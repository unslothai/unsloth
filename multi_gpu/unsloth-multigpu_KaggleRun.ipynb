{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":289904,"sourceType":"modelInstanceVersion","modelInstanceId":248365,"modelId":269879},{"sourceId":289910,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":248371,"modelId":269885},{"sourceId":289933,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":248392,"modelId":269908}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import unsloth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import unsloth\nfrom unsloth import FastLanguageModel\nfrom unsloth import is_bfloat16_supported\nfrom unsloth.chat_templates import get_chat_template, train_on_responses_only\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import unsloth\nfrom unsloth import FastLanguageModel\nfrom unsloth import is_bfloat16_supported\nfrom unsloth.chat_templates import get_chat_template, train_on_responses_only,standardize_sharegpt\n\nimport os\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom accelerate import Accelerator\n\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" #Select Which devices to use. Or, comment if you want to use all GPUs.\nos.environ[\"UNSLOTH_RETURN_LOGITS\"] = \"1\"\naccelerator = Accelerator()\n\n\n# to be use in the terminal\n#   accelerate launch --config_file acc_config.yaml unsloth_Accelerate.py\n\n\ndevice = accelerator.device\n\n\ndef load_model(model_path):\n    max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n    device_index = Accelerator().process_index\n    device_map = {\"\": device_index}\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = model_path,\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n        device_map=device_map,\n        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n    )\n\n    return model, tokenizer\n\ndef model_LoRA(base_model):\n    model = FastLanguageModel.get_peft_model(\n        base_model,\n        r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128, USE 8\n        # target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        #                 ],\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\",],\n        lora_alpha = 16, # USE 32\n        lora_dropout = 0, # Supports any, but = 0 is optimized USE 0.3\n        bias = \"none\",    # Supports any, but = \"none\" is optimized\n        # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n        # use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n        use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n        random_state = 3407,\n        use_rslora = False,  # We support rank stabilized LoRA\n        loftq_config = None, # And LoftQ\n    )\n\n\n    return model\n\n### FUNCTION CALLING\n# model_path = \"unsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit\"\nmodel_path = \"unsloth/Qwen2.5-Coder-7B-Instruct\"\nmodel, tokenizer = load_model(model_path=model_path)\n\n\n##APPLY LORA\nmodel = model_LoRA(base_model=model)\n\ndef load_data(data_path):\n    dataset_train = load_dataset(data_path, split = \"train\")\n\n    return dataset_train\n\ndata_path = \"mlabonne/FineTome-100k\"\ndataset_train = load_data(data_path=data_path)\n\n\n\ndef split_train_val(dataset):\n    # Split training dataset into train and validation sets (80-20 split)\n    train_test_split = dataset.train_test_split(test_size=0.1238, seed=42)\n    dataset_train = train_test_split[\"train\"]\n    dataset_val = train_test_split[\"test\"]  # This becomes the validation set\n\n    return dataset_train, dataset_val\n\ndataset_train, dataset_val = split_train_val(dataset_train)\n\n# Initialize the tokenizer with Qwen2.5 chat template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template=\"qwen-2.5\",\n)\n\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\n\n\ndataset_train = standardize_sharegpt(dataset_train)\ndataset_val = standardize_sharegpt(dataset_val)\n\n\n\n# Apply formatting\ndataset_train = dataset_train.map(formatting_prompts_func, batched=True,)\ndataset_val = dataset_val.map(formatting_prompts_func, batched=True,)\n\n\ndef def_trainer(model, tokenizer, dataset_train):\n\n    # Create the TrainingArguments\n    training_args = TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=1,\n        warmup_steps=5,\n        max_steps=30,\n        # num_train_epochs=20,  # Set to 20 epochs\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"paged_adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        report_to=\"none\",  # Disable WandB or other reporting\n        greater_is_better=False,\n        # load_best_model_at_end=True,\n        ddp_find_unused_parameters=False,\n    )\n\n    # Define the Trainer with the given parameters\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=dataset_train,\n        eval_dataset=dataset_val,\n        dataset_text_field=\"text\",\n        max_seq_length=2048,\n        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n        dataset_num_proc=4,\n        packing=False,\n        args=training_args,\n    )\n\n    trainer = train_on_responses_only(\n        trainer,\n        instruction_part = \"<|im_start|>user\\n\",\n        response_part = \"<|im_start|>assistant\\n\",\n    )\n\n    tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])\n\n    space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n    tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])\n\n     \n    # If the model is wrapped in DDP, access the underlying module:\n    if hasattr(trainer.model, \"module\") and hasattr(trainer.model.module, \"_set_static_graph\"):\n        trainer.model.module._set_static_graph()\n    elif hasattr(trainer.model, \"_set_static_graph\"):\n        trainer.model._set_static_graph()\n    return trainer\n\ntrainer = def_trainer(model=model,tokenizer=tokenizer,dataset_train=dataset_train)\ntrainer_stats = trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch /kaggle/input/unsloth_multigpu/transformers/default/1/unsloth_Accelerate.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T17:59:37.857661Z","iopub.execute_input":"2025-03-17T17:59:37.858095Z","iopub.status.idle":"2025-03-17T18:05:34.353596Z","shell.execute_reply.started":"2025-03-17T17:59:37.858061Z","shell.execute_reply":"2025-03-17T18:05:34.352651Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n2025-03-17 17:59:48.524656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-17 17:59:48.547975: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-17 17:59:48.554892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-03-17 17:59:48.981412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-17 17:59:49.004009: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-17 17:59:49.011122: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n[W317 17:59:59.030464367 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n[W317 17:59:59.323771138 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n==((====))==  Unsloth 2025.3.14: Fast Qwen2 patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n==((====))==  Unsloth 2025.3.14: Fast Qwen2 patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth 2025.3.14 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\nUnsloth 2025.3.14 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87620/87620 [00:09<00:00, 9613.59 examples/s]\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87620/87620 [00:08<00:00, 10371.27 examples/s]\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12380/12380 [00:01<00:00, 9713.14 examples/s]\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12380/12380 [00:01<00:00, 10367.53 examples/s]\nUnsloth: Tokenizing [\"text\"] (num_proc=4): 100%|â–ˆ| 87620/87620 [02:28<00:00, 591\nUnsloth: Tokenizing [\"text\"] (num_proc=4): 100%|â–ˆ| 87620/87620 [02:29<00:00, 586\nUnsloth: Tokenizing [\"text\"] (num_proc=4): 100%|â–ˆ| 12380/12380 [00:22<00:00, 539\nUnsloth: Tokenizing [\"text\"] (num_proc=4): 100%|â–ˆ| 12380/12380 [00:23<00:00, 535\nMap (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87620/87620 [00:43<00:00, 2018.07 examples/s]\nMap (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87620/87620 [00:43<00:00, 2000.04 examples/s]\nMap (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12380/12380 [00:06<00:00, 1782.54 examples/s]\nMap (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12380/12380 [00:06<00:00, 1886.16 examples/s]\nUnsloth is running with multi GPUs - the effective batch size is multiplied by 2\nUnsloth is running with multi GPUs - the effective batch size is multiplied by 2\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 87,620 | Num Epochs = 1 | Total steps = 30\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n\\        /    Data Parallel GPUs = 2 | Total batch size (1 x 1 x 2) = 2\n \"-____-\"     Trainable parameters = 40,370,176/4,393,342,464 (0.92% trained)\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 87,620 | Num Epochs = 1 | Total steps = 30\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n\\        /    Data Parallel GPUs = 2 | Total batch size (1 x 1 x 2) = 2\n \"-____-\"     Trainable parameters = 40,370,176/4,393,342,464 (0.92% trained)\n{'loss': 0.4845, 'grad_norm': 0.18897125124931335, 'learning_rate': 4e-05, 'epoch': 0.0}\n  3%|â–ˆâ–                                          | 1/30 [00:03<01:42,  3.53s/it]Unsloth: Will smartly offload gradients to save VRAM!\n{'loss': 0.6444, 'grad_norm': 0.20603598654270172, 'learning_rate': 8e-05, 'epoch': 0.0}\n  7%|â–ˆâ–ˆâ–‰                                         | 2/30 [00:06<01:25,  3.06s/it]Unsloth: Will smartly offload gradients to save VRAM!\n{'loss': 0.8779, 'grad_norm': 0.13421091437339783, 'learning_rate': 0.00012, 'epoch': 0.0}\n{'loss': 0.9832, 'grad_norm': 0.235795259475708, 'learning_rate': 0.00016, 'epoch': 0.0}\n{'loss': 1.2742, 'grad_norm': 0.18406198918819427, 'learning_rate': 0.0002, 'epoch': 0.0}\n{'loss': 0.7446, 'grad_norm': 0.15907271206378937, 'learning_rate': 0.000192, 'epoch': 0.0}\n{'loss': 0.5815, 'grad_norm': 0.2432064712047577, 'learning_rate': 0.00018400000000000003, 'epoch': 0.0}\n{'loss': 0.6084, 'grad_norm': 0.13815373182296753, 'learning_rate': 0.00017600000000000002, 'epoch': 0.0}\n{'loss': 0.5421, 'grad_norm': 0.15123797953128815, 'learning_rate': 0.000168, 'epoch': 0.0}\n{'loss': 0.7307, 'grad_norm': 0.16218987107276917, 'learning_rate': 0.00016, 'epoch': 0.0}\n{'loss': 0.5039, 'grad_norm': 0.2139032483100891, 'learning_rate': 0.000152, 'epoch': 0.0}\n{'loss': 0.5554, 'grad_norm': 0.19900557398796082, 'learning_rate': 0.000144, 'epoch': 0.0}\n{'loss': 0.336, 'grad_norm': 0.13795924186706543, 'learning_rate': 0.00013600000000000003, 'epoch': 0.0}\n{'loss': 0.412, 'grad_norm': 0.2006906270980835, 'learning_rate': 0.00012800000000000002, 'epoch': 0.0}\n{'loss': 0.5774, 'grad_norm': 0.18523487448692322, 'learning_rate': 0.00012, 'epoch': 0.0}\n{'loss': 0.3778, 'grad_norm': 0.19453318417072296, 'learning_rate': 0.00011200000000000001, 'epoch': 0.0}\n{'loss': 0.5534, 'grad_norm': 0.208096444606781, 'learning_rate': 0.00010400000000000001, 'epoch': 0.0}\n{'loss': 0.7437, 'grad_norm': 0.22127510607242584, 'learning_rate': 9.6e-05, 'epoch': 0.0}\n{'loss': 0.7847, 'grad_norm': 0.46526220440864563, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.0}\n{'loss': 0.528, 'grad_norm': 0.15774674713611603, 'learning_rate': 8e-05, 'epoch': 0.0}\n{'loss': 0.5964, 'grad_norm': 0.1947704255580902, 'learning_rate': 7.2e-05, 'epoch': 0.0}\n{'loss': 0.557, 'grad_norm': 0.2676469087600708, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.0}\n{'loss': 0.65, 'grad_norm': 0.28408560156822205, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.0}\n{'loss': 0.6813, 'grad_norm': 0.28477248549461365, 'learning_rate': 4.8e-05, 'epoch': 0.0}\n{'loss': 0.57, 'grad_norm': 0.20220676064491272, 'learning_rate': 4e-05, 'epoch': 0.0}\n{'loss': 0.8314, 'grad_norm': 0.19951918721199036, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.0}\n{'loss': 0.6541, 'grad_norm': 0.261885404586792, 'learning_rate': 2.4e-05, 'epoch': 0.0}\n{'loss': 0.7996, 'grad_norm': 0.25769203901290894, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}\n{'loss': 0.759, 'grad_norm': 0.18749664723873138, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n{'loss': 0.8014, 'grad_norm': 0.2867184579372406, 'learning_rate': 0.0, 'epoch': 0.0}\n{'train_runtime': 77.2772, 'train_samples_per_second': 0.776, 'train_steps_per_second': 0.388, 'train_loss': 0.6581262121597926, 'epoch': 0.0}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:17<00:00,  2.58s/it]\n[rank0]:[W317 18:05:30.913991129 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n","output_type":"stream"}],"execution_count":1}]}