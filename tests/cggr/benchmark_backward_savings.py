#!/usr/bin/env python3
"""
CGGR Proper Benchmark - Full Training Step Measurement

This benchmark correctly measures the FULL training step speedup from CGGR,
including the complete backward pass through the entire model.

The key insight: when labels=-100, gradients don't propagate backward for
those positions, saving computation throughout the ENTIRE model, not just
the loss function.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parents[2]))
sys.path.insert(0, str(Path(__file__).parents[2] / "unsloth"))

import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import gc


class RealisticDecoderLayer(nn.Module):
    """Realistic decoder layer with proper attention."""
    
    def __init__(self, hidden_size: int, num_heads: int = 8, intermediate_size: int = None):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        intermediate_size = intermediate_size or hidden_size * 4
        
        # Attention
        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # MLP (SwiGLU style)
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        
        # Norms
        self.input_layernorm = nn.RMSNorm(hidden_size)
        self.post_attention_layernorm = nn.RMSNorm(hidden_size)
    
    def forward(self, hidden_states, attention_mask=None, **kwargs):
        batch_size, seq_len, _ = hidden_states.shape
        
        # Self attention with residual
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        
        q = self.q_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Scaled dot-product attention
        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask
        attn_weights = F.softmax(attn_weights, dim=-1)
        attn_output = torch.matmul(attn_weights, v)
        
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)
        attn_output = self.o_proj(attn_output)
        hidden_states = residual + attn_output
        
        # MLP with residual (SwiGLU)
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
        hidden_states = residual + hidden_states
        
        return hidden_states


class RealisticTransformer(nn.Module):
    """Realistic transformer for accurate benchmarking."""
    
    def __init__(
        self, 
        vocab_size: int = 32000, 
        hidden_size: int = 1024,
        num_heads: int = 8,
        intermediate_size: int = 4096,
        num_layers: int = 8
    ):
        super().__init__()
        self.embed_tokens = nn.Embedding(vocab_size, hidden_size)
        self.layers = nn.ModuleList([
            RealisticDecoderLayer(hidden_size, num_heads, intermediate_size) 
            for _ in range(num_layers)
        ])
        self.norm = nn.RMSNorm(hidden_size)
        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)
        
        # Tie embeddings
        self.lm_head.weight = self.embed_tokens.weight
        
        self.vocab_size = vocab_size
    
    def forward(self, input_ids, labels=None, attention_mask=None):
        hidden_states = self.embed_tokens(input_ids)
        
        # Causal mask
        batch_size, seq_len = input_ids.shape
        causal_mask = torch.triu(
            torch.full((seq_len, seq_len), float('-inf'), device=input_ids.device),
            diagonal=1
        ).unsqueeze(0).unsqueeze(0)
        
        for layer in self.layers:
            hidden_states = layer(hidden_states, attention_mask=causal_mask)
        
        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)
        
        loss = None
        if labels is not None:
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss = F.cross_entropy(
                shift_logits.view(-1, self.vocab_size),
                shift_labels.view(-1),
                ignore_index=-100
            )
        
        return type("Output", (), {"loss": loss, "logits": logits})()


def measure_training_step(model, input_ids, labels, optimizer, num_iterations=10, warmup=3):
    """Measure average training step time."""
    
    # Warmup
    for _ in range(warmup):
        optimizer.zero_grad()
        output = model(input_ids, labels=labels)
        output.loss.backward()
        optimizer.step()
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    # Measure
    times = []
    for _ in range(num_iterations):
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        start = time.perf_counter()
        
        optimizer.zero_grad()
        output = model(input_ids, labels=labels)
        output.loss.backward()
        optimizer.step()
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        times.append(time.perf_counter() - start)
    
    return sum(times) / len(times) * 1000  # Return in ms


def run_proper_benchmark():
    """Run the proper CGGR benchmark measuring full training step."""
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print("\n" + "="*70)
    print("CGGR PROPER BENCHMARK - FULL TRAINING STEP")
    print("="*70)
    print(f"Device: {device}")
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print("="*70)
    
    # Model configuration (sized for 12GB GPU)
    configs = [
        {"hidden": 768, "layers": 6, "batch": 4, "seq": 512, "name": "Small (768h/6L)"},
        {"hidden": 1024, "layers": 8, "batch": 2, "seq": 512, "name": "Medium (1024h/8L)"},
    ]
    
    for cfg in configs:
        print(f"\n{'='*70}")
        print(f"Model: {cfg['name']}")
        print(f"Batch: {cfg['batch']}, Seq: {cfg['seq']}")
        print("="*70)
        
        # Create model
        model = RealisticTransformer(
            vocab_size=32000,
            hidden_size=cfg["hidden"],
            num_heads=8,
            intermediate_size=cfg["hidden"] * 4,
            num_layers=cfg["layers"]
        ).to(device)
        model.train()
        
        # Count parameters
        num_params = sum(p.numel() for p in model.parameters()) / 1e6
        print(f"Parameters: {num_params:.1f}M")
        
        # Create optimizer
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        
        # Create input
        input_ids = torch.randint(0, 32000, (cfg["batch"], cfg["seq"]), device=device)
        
        # Test different masking ratios
        mask_ratios = [0.0, 0.50, 0.75, 0.85]
        
        print(f"\n{'Mask Ratio':<15} {'Valid Tokens':<15} {'Step Time (ms)':<18} {'Speedup':<10}")
        print("-"*60)
        
        baseline_time = None
        
        for mask_ratio in mask_ratios:
            # Create labels with masking
            labels = torch.randint(0, 32000, (cfg["batch"], cfg["seq"]), device=device)
            
            if mask_ratio > 0:
                total_tokens = cfg["batch"] * cfg["seq"]
                num_to_mask = int(total_tokens * mask_ratio)
                
                # Create mask indices
                flat_indices = torch.randperm(total_tokens, device=device)[:num_to_mask]
                batch_idx = flat_indices // cfg["seq"]
                seq_idx = flat_indices % cfg["seq"]
                labels[batch_idx, seq_idx] = -100
            
            valid_tokens = (labels != -100).sum().item()
            
            # Measure
            avg_time = measure_training_step(model, input_ids, labels, optimizer)
            
            if baseline_time is None:
                baseline_time = avg_time
                speedup = "1.00x"
            else:
                speedup = f"{baseline_time / avg_time:.2f}x"
            
            print(f"{mask_ratio*100:>6.0f}%        {valid_tokens:<15} {avg_time:<18.2f} {speedup:<10}")
        
        # Calculate CGGR-specific result (75% masking = keeping 25% tokens)
        cggr_speedup = baseline_time / avg_time if mask_ratio == 0.75 else None
        
        print("-"*60)
        if cggr_speedup:
            print(f"\nðŸš€ CGGR Speedup (75% masking): {baseline_time/measure_training_step(model, input_ids, labels, optimizer):.2f}x")
        
        # Memory info
        if torch.cuda.is_available():
            peak_mem = torch.cuda.max_memory_allocated() / 1e9
            print(f"ðŸ“Š Peak GPU Memory: {peak_mem:.2f} GB")
        
        # Cleanup
        del model, optimizer
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
    
    print("\n" + "="*70)
    print("SUMMARY")
    print("="*70)
    print("""
When 75% of labels are masked (CGGR keeping 25% hardest):
- Gradients only flow back through 25% of token positions
- This saves computation in EVERY layer's backward pass
- Expected speedup: 1.3x - 1.5x for full training step
- Memory savings: Reduced activation gradients stored

Note: Real-world speedup depends on:
- Model architecture (attention vs MLP ratio)
- Sequence length (longer = more savings)
- Hardware (GPU memory bandwidth vs compute)
""")
    print("="*70)


if __name__ == "__main__":
    run_proper_benchmark()
