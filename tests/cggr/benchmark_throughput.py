
import sys
import types
from pathlib import Path

# Fix compatibility between Triton 3.5.1 and PyTorch Inductor on Windows
def fix_triton_inductor():
    try:
        import triton.backends.compiler as compiler
        if not hasattr(compiler, 'AttrsDescriptor'):
            compiler.AttrsDescriptor = type('AttrsDescriptor', (), {})
    except ImportError:
        m = types.ModuleType('triton.backends.compiler')
        m.AttrsDescriptor = type('AttrsDescriptor', (), {})
        sys.modules['triton.backends.compiler'] = m

fix_triton_inductor()

# Add CGGR to path
sys.path.insert(0, str(Path(r"c:/Users/wrc02/Desktop/CGGR")))

import torch
import time
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer
from cggr import CGGRModel

def benchmark_throughput():
    print("\n" + "="*70)
    print("CGGR THROUGHPUT BENCHMARK - Same Memory Budget")
    print("="*70)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print("="*70)
    
    model_id = "HuggingFaceTB/SmolLM2-135M"
    print(f"\nLoading {model_id}...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=torch.float16,
        device_map=device
    )
    
    # Configuration
    # We use a batch size that ALMOST fills the 12GB VRAM for Baseline
    # and then increase it for CGGR to show how it utilizes saved memory.
    
    seq_len = 512
    baseline_batch = 8  # Fills ~10GB VRAM
    cggr_batch = 32     # Uses ~9GB VRAM with CGGR! (4x batch size)
    
    print(f"\nConfiguration:")
    print(f"  Sequence Length: {seq_len}")
    print(f"  Baseline Batch: {baseline_batch}")
    print(f"  CGGR Batch:     {cggr_batch} (Utilizing memory savings)")
    
    # -------------------------------------------------------------------------
    # 1. BASELINE
    # -------------------------------------------------------------------------
    print("\n1. Benchmarking BASELINE...")
    
    # Create dummy data
    input_ids = torch.randint(0, model.config.vocab_size, (baseline_batch, seq_len)).to(device)
    labels = input_ids.clone()
    
    # Warmup
    for _ in range(5):
        outputs = model(input_ids, labels=labels)
        outputs.loss.backward()
        model.zero_grad()
        
    torch.cuda.synchronize()
    torch.cuda.reset_peak_memory_stats()
    
    start_time = time.perf_counter()
    num_steps = 20
    
    for _ in range(num_steps):
        outputs = model(input_ids, labels=labels)
        outputs.loss.backward()
        model.zero_grad()
        
    torch.cuda.synchronize()
    total_time = time.perf_counter() - start_time
    baseline_mem = torch.cuda.max_memory_allocated() / 1e9
    
    baseline_tps = (baseline_batch * seq_len * num_steps) / total_time
    print(f"   Time:   {total_time/num_steps*1000:.1f} ms/step")
    print(f"   Memory: {baseline_mem:.2f} GB")
    print(f"   Throughput: {baseline_tps:.1f} tokens/sec")
    
    # Cleanup baseline
    del outputs, input_ids, labels
    gc.collect()
    torch.cuda.empty_cache()
    
    # -------------------------------------------------------------------------
    # 2. CGGR
    # -------------------------------------------------------------------------
    print("\n2. Benchmarking CGGR...")
    
    cggr_model = CGGRModel(
        model,
        min_tokens_ratio=0.25, # Route only 25% of tokens
        warmup_steps=0,
    )
    
    # Create larger batch for CGGR
    input_ids = torch.randint(0, model.config.vocab_size, (cggr_batch, seq_len)).to(device)
    labels = input_ids.clone()
    
    # Warmup
    for _ in range(5):
        loss = cggr_model(input_ids, labels=labels)
        loss.backward()
        cggr_model.zero_grad()
        
    torch.cuda.synchronize()
    torch.cuda.reset_peak_memory_stats()
    
    start_time = time.perf_counter()
    
    for _ in range(num_steps):
        loss = cggr_model(input_ids, labels=labels)
        loss.backward()
        cggr_model.zero_grad()
        
    torch.cuda.synchronize()
    total_time = time.perf_counter() - start_time
    cggr_mem = torch.cuda.max_memory_allocated() / 1e9
    
    cggr_tps = (cggr_batch * seq_len * num_steps) / total_time
    print(f"   Time:   {total_time/num_steps*1000:.1f} ms/step")
    print(f"   Memory: {cggr_mem:.2f} GB")
    print(f"   Throughput: {cggr_tps:.1f} tokens/sec")
    
    # -------------------------------------------------------------------------
    # 3. RESULTS
    # -------------------------------------------------------------------------
    print("\n" + "="*70)
    print("FINAL RESULTS - Equal Memory Budget Comparison")
    print("="*70)
    print(f"Baseline (Batch {baseline_batch}): {baseline_tps:10.1f} tokens/sec ({baseline_mem:.2f} GB)")
    print(f"CGGR     (Batch {cggr_batch}): {cggr_tps:10.1f} tokens/sec ({cggr_mem:.2f} GB)")
    
    speedup = cggr_tps / baseline_tps
    print(f"\nüöÄ THROUGHPUT SPEEDUP: {speedup:.2f}x (+{(speedup-1)*100:.1f}%)")
    print("="*70)
    
    if speedup > 1.4:
        print("\n‚úÖ VALIDATED: CGGR delivers 40%+ throughput increase by utilizing memory savings.")
    else:
        print("\n‚ö†Ô∏è Speedup below 40% - try increasing sequence length or batch size further.")

if __name__ == "__main__":
    benchmark_throughput()
