{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4800978d",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rolandtannous/unsloth_scratchpad/blob/main/nb/gemma3-vision-test/gemma3_vision_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83fc6ff-29f2-4a57-9ce4-59d91b43ac3d",
   "metadata": {
    "editable": true,
    "id": "e83fc6ff-29f2-4a57-9ce4-59d91b43ac3d",
    "outputId": "856c05b4-d520-4dfe-d8e4-d597ae969839",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "forward 1 working and being replaced\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-18 05:14:05 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca3f87-5973-4d2f-88e8-6de6e05dc4fa",
   "metadata": {
    "id": "9bca3f87-5973-4d2f-88e8-6de6e05dc4fa"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ba9d1e-0ad3-43b7-9a94-fa58f6b1a4f2",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "905e0b0bcab74845a4858b6de6199c45",
      "61e16790b19f496794d91eecc10d369a",
      "62ab7989b1bd4179aaff9b09515cad6d"
     ]
    },
    "id": "12ba9d1e-0ad3-43b7-9a94-fa58f6b1a4f2",
    "outputId": "0561eb67-9383-4ecc-ea0d-6cca4e76d006"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0110526de74ff589e959db6ee94af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc92ef4f4e944312a3246f0dda17af31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7f25d097e14722ae5dee2fd011cebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lbourdois/OCR-liboaccn-OPUS-MIT-5M-clean\", 'en', split=\"train\")\n",
    "# To select the first 2000 examples\n",
    "train_dataset = dataset.select(range(2000))\n",
    "\n",
    "# To select the next 200 examples for evaluation\n",
    "eval_dataset = dataset.select(range(2000, 2200))\n",
    "\n",
    "# Convert dataset to OAI messages       \n",
    "def format_data(sample):\n",
    "    return {\"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": system_message + sample[\"question\"],\n",
    "                        },{\n",
    "                            \"type\": \"image\",\n",
    "                            \"image\": sample[\"image\"].convert(\"RGB\"),\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": sample[\"answer\"]}],\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "\n",
    "system_message = \"You are an expert french ocr system.\"\n",
    "# Convert dataset to OAI messages\n",
    "# need to use list comprehension to keep Pil.Image type, .mape convert image to bytes\n",
    "train_dataset = [format_data(sample) for sample in train_dataset]\n",
    "eval_dataset = [format_data(sample) for sample in eval_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28489f4-82e2-4171-a96b-344bf8e39a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from jiwer import wer, cer\n",
    "from typing import Any, List, Dict, Tuple, Optional\n",
    "#import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class OCRModelEvaluator:\n",
    "    \"\"\"\n",
    "    A comprehensive OCR model evaluator that supports Gemma3 and other vision-language models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the OCR evaluator.\"\"\"\n",
    "        self.model_comparison_results = {}\n",
    "\n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        model: Any,\n",
    "        processor: Any,\n",
    "        dataset: List[Dict],\n",
    "        output_dir: str = \"ocr_evaluation_results\",\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.8,\n",
    "        top_p: float = 1.0,\n",
    "        top_k: int = 64,\n",
    "        do_sample: bool = True,\n",
    "        verbose: bool = True\n",
    "    ) -> Tuple[Optional[float], Optional[float]]:\n",
    "        \"\"\"\n",
    "        Evaluate a Gemma3 model on an OCR dataset.\n",
    "        \"\"\"\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize results storage\n",
    "        results = []\n",
    "\n",
    "        # Process each sample in the dataset\n",
    "        for i, sample in enumerate(tqdm(dataset, desc=\"Evaluating OCR performance\", disable=not verbose)):\n",
    "            try:\n",
    "                # Extract components from sample\n",
    "                messages = sample['messages']\n",
    "\n",
    "                # Get ground truth, image, and question, input_messages\n",
    "                ground_truth, image, question, input_messages = self._extract_sample_components(\n",
    "                    messages, i, verbose\n",
    "                )\n",
    "\n",
    "                if ground_truth is None or image is None or question is None:\n",
    "                    continue\n",
    "\n",
    "                # Generate model response\n",
    "                generated_response = self._generate_response(\n",
    "                    model, processor, input_messages, max_new_tokens, temperature, top_p, top_k, do_sample\n",
    "                )\n",
    "\n",
    "                # Calculate metrics\n",
    "                word_error = wer(ground_truth, generated_response)\n",
    "                char_error = cer(ground_truth, generated_response)\n",
    "\n",
    "                # Save individual result\n",
    "                self._save_individual_result(\n",
    "                    output_dir, i, question, generated_response, ground_truth, word_error, char_error\n",
    "                )\n",
    "\n",
    "                # Store results for summary\n",
    "                results.append({\n",
    "                    'sample_id': i,\n",
    "                    'wer': word_error,\n",
    "                    'cer': char_error,\n",
    "                    'model_output': generated_response.strip(),\n",
    "                    'ground_truth': ground_truth,\n",
    "                    'question': question\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Error processing sample {i}: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "\n",
    "        # Generate summary report\n",
    "        return self._generate_summary_report(results, output_dir, verbose)\n",
    "\n",
    "    def _extract_sample_components(\n",
    "        self,\n",
    "        messages: List[Dict],\n",
    "        sample_idx: int,\n",
    "        verbose: bool\n",
    "    ) -> Tuple[Optional[str], Optional[Any], Optional[str], List[Dict]]:\n",
    "        \"\"\"Extract ground truth, image, question, and input messages from sample.\"\"\"\n",
    "\n",
    "        # Extract system message (if present)\n",
    "        system_message = next((msg for msg in messages if msg['role'] == 'system'), None)\n",
    "\n",
    "        # Extract user message with the image and question\n",
    "        user_message = next((msg for msg in messages if msg['role'] == 'user'), None)\n",
    "        if not user_message:\n",
    "            if verbose:\n",
    "                print(f\"Skipping sample {sample_idx}: No user message found\")\n",
    "            return None, None, None, []\n",
    "\n",
    "        # Extract assistant message with ground truth\n",
    "        assistant_message = next((msg for msg in messages if msg['role'] == 'assistant'), None)\n",
    "        if not assistant_message:\n",
    "            if verbose:\n",
    "                print(f\"Skipping sample {sample_idx}: No assistant message (ground truth) found\")\n",
    "            return None, None, None, []\n",
    "\n",
    "        # Extract ground truth text\n",
    "        ground_truth = None\n",
    "        for content_item in assistant_message['content']:\n",
    "            if content_item['type'] == 'text':\n",
    "                ground_truth = content_item['text']\n",
    "                break\n",
    "\n",
    "        if not ground_truth:\n",
    "            if verbose:\n",
    "                print(f\"Skipping sample {sample_idx}: No text found in assistant message\")\n",
    "            return None, None, None, []\n",
    "\n",
    "        # Extract image and question from user message\n",
    "        image = None\n",
    "        question = None\n",
    "\n",
    "        for content_item in user_message['content']:\n",
    "            if content_item['type'] == 'image':\n",
    "                image = content_item['image']\n",
    "                # Ensure image is in RGB format\n",
    "                if hasattr(image, 'convert'):\n",
    "                    image = image.convert('RGB')\n",
    "            elif content_item['type'] == 'text':\n",
    "                question = content_item['text']\n",
    "\n",
    "        if not image:\n",
    "            if verbose:\n",
    "                print(f\"Skipping sample {sample_idx}: No image found in user message\")\n",
    "            return None, None, None, []\n",
    "\n",
    "        if not question:\n",
    "            if verbose:\n",
    "                print(f\"Skipping sample {sample_idx}: No question found in user message\")\n",
    "            return None, None, None, []\n",
    "\n",
    "        # Construct messages for the model input (excluding assistant message)\n",
    "        input_messages = []\n",
    "        if system_message:\n",
    "            input_messages.append(system_message)\n",
    "        input_messages.append(user_message)\n",
    "\n",
    "        return ground_truth, image, question, input_messages\n",
    "\n",
    "    def _process_vision_info(self, messages: List[Dict]) -> List[Image.Image]:\n",
    "        \"\"\"Extract images from messages in Gemma3 format.\"\"\"\n",
    "        image_inputs = []\n",
    "        # Iterate through each conversation\n",
    "        for msg in messages:\n",
    "            # Get content (ensure it's a list)\n",
    "            content = msg.get(\"content\", [])\n",
    "            if not isinstance(content, list):\n",
    "                content = [content]\n",
    "\n",
    "            # Check each content element for images\n",
    "            for element in content:\n",
    "                if isinstance(element, dict) and (\n",
    "                    \"image\" in element or element.get(\"type\") == \"image\"\n",
    "                ):\n",
    "                    # Get the image and convert to RGB\n",
    "                    if \"image\" in element:\n",
    "                        image = element[\"image\"]\n",
    "                    else:\n",
    "                        image = element\n",
    "                    if hasattr(image, 'convert'):\n",
    "                        image_inputs.append(image.convert(\"RGB\"))\n",
    "                    else:\n",
    "                        image_inputs.append(image)\n",
    "        return image_inputs\n",
    "\n",
    "    def _generate_response(\n",
    "        self,\n",
    "        model: Any,\n",
    "        processor: Any,\n",
    "        input_messages: List[Dict],\n",
    "        max_new_tokens: int,\n",
    "        temperature: float,\n",
    "        top_p: float,\n",
    "        top_k: int,\n",
    "        do_sample: bool,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate response from the Gemma3 model using the official approach.\"\"\"\n",
    "\n",
    "        # Apply chat template to convert messages to text\n",
    "        text = processor.apply_chat_template(\n",
    "            input_messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Process the images using the official vision processing function\n",
    "        image_inputs = self._process_vision_info(input_messages)\n",
    "\n",
    "        # Tokenize the text and process the images\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Move the inputs to the device\n",
    "        inputs = inputs.to(model.device)\n",
    "\n",
    "        # Set up stop tokens (following the official implementation)\n",
    "        stop_token_ids = [\n",
    "            processor.tokenizer.eos_token_id, \n",
    "            processor.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "        ]\n",
    "\n",
    "        # Generate the output with proper parameters\n",
    "        with torch.inference_mode():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=max_new_tokens, \n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                do_sample=do_sample, \n",
    "                temperature=temperature, \n",
    "                eos_token_id=stop_token_ids,\n",
    "                disable_compile=True  # Following official implementation\n",
    "            )\n",
    "\n",
    "        # Trim the generation (remove input tokens)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        # Decode the generated text\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, \n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        return output_text[0] if output_text else \"\"\n",
    "\n",
    "    def _save_individual_result(\n",
    "        self,\n",
    "        output_dir: str,\n",
    "        sample_idx: int,\n",
    "        question: str,\n",
    "        generated_response: str,\n",
    "        ground_truth: str,\n",
    "        word_error: float,\n",
    "        char_error: float\n",
    "    ):\n",
    "        \"\"\"Save individual sample result to file.\"\"\"\n",
    "        output_file = os.path.join(output_dir, f\"sample_{sample_idx}.txt\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Sample {sample_idx}\\n\")\n",
    "            f.write(f\"Question: {question}\\n\\n\")\n",
    "            f.write(f\"Model output:\\n{generated_response.strip()}\\n\\n\")\n",
    "            f.write(f\"Ground truth:\\n{ground_truth}\\n\\n\")\n",
    "            f.write(f\"WER: {word_error:.4f}, CER: {char_error:.4f}\")\n",
    "\n",
    "    def _generate_summary_report(\n",
    "        self,\n",
    "        results: List[Dict],\n",
    "        output_dir: str,\n",
    "        verbose: bool\n",
    "    ) -> Tuple[Optional[float], Optional[float]]:\n",
    "        \"\"\"Generate and save summary report.\"\"\"\n",
    "        if not results:\n",
    "            if verbose:\n",
    "                print(\"No results to summarize.\")\n",
    "            return None, None\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "        # Calculate overall averages\n",
    "        avg_wer = df['wer'].mean()\n",
    "        avg_cer = df['cer'].mean()\n",
    "\n",
    "        # Save average metrics\n",
    "        with open(os.path.join(output_dir, \"avg_metrics.txt\"), 'w') as f:\n",
    "            f.write(f\"Average WER: {avg_wer:.4f}\\n\")\n",
    "            f.write(f\"Average CER: {avg_cer:.4f}\\n\")\n",
    "\n",
    "        # Save detailed results\n",
    "        df.to_csv(os.path.join(output_dir, \"detailed_results.csv\"), index=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\nResults Summary:\")\n",
    "            print(f\"Average WER: {avg_wer:.4f}\")\n",
    "            print(f\"Average CER: {avg_cer:.4f}\")\n",
    "            print(f\"\\nDetailed results saved to {output_dir}/\")\n",
    "\n",
    "        return avg_wer, avg_cer\n",
    "\n",
    "    def add_to_comparison(self, model_name: str, wer: float, cer: float):\n",
    "        \"\"\"Add model results to the comparison tracker.\"\"\"\n",
    "        self.model_comparison_results[model_name] = {\n",
    "            \"wer\": wer,\n",
    "            \"cer\": cer\n",
    "        }\n",
    "\n",
    "    def print_model_comparison(self, save_csv: bool = True, save_plot: bool = True) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Print a comparison of all models evaluated so far.\"\"\"\n",
    "        if not self.model_comparison_results:\n",
    "            print(\"No model results available for comparison\")\n",
    "            return None\n",
    "\n",
    "        print(\"\\n==== MODEL COMPARISON REPORT ====\")\n",
    "\n",
    "        # Create a comparison dataframe\n",
    "        comparison_df = pd.DataFrame({\n",
    "            \"Model\": list(self.model_comparison_results.keys()),\n",
    "            \"WER\": [results[\"wer\"] for results in self.model_comparison_results.values()],\n",
    "            \"CER\": [results[\"cer\"] for results in self.model_comparison_results.values()]\n",
    "        })\n",
    "\n",
    "        # Sort by WER (best performance first)\n",
    "        comparison_df = comparison_df.sort_values(\"WER\")\n",
    "\n",
    "        # Display the comparison table\n",
    "        print(\"\\nComparison Table (sorted by WER):\")\n",
    "        print(comparison_df.to_string(index=False))\n",
    "\n",
    "        # Save the comparison table\n",
    "        if save_csv:\n",
    "            comparison_file = \"model_comparison_results.csv\"\n",
    "            comparison_df.to_csv(comparison_file, index=False)\n",
    "            print(f\"\\nComparison table saved to {comparison_file}\")\n",
    "\n",
    "        # Generate a bar chart visualization\n",
    "        if save_plot:\n",
    "            self._create_comparison_plot(comparison_df)\n",
    "\n",
    "        return comparison_df\n",
    "\n",
    "    def _create_comparison_plot(self, comparison_df: pd.DataFrame):\n",
    "        \"\"\"Create and save comparison plot.\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot WER\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(comparison_df[\"Model\"], comparison_df[\"WER\"], color='skyblue')\n",
    "        plt.title('Word Error Rate Comparison')\n",
    "        plt.ylabel('WER (lower is better)')\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        # Plot CER\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.bar(comparison_df[\"Model\"], comparison_df[\"CER\"], color='lightgreen')\n",
    "        plt.title('Character Error Rate Comparison')\n",
    "        plt.ylabel('CER (lower is better)')\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('ocr_model_comparison.png')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nVisualization saved to ocr_model_comparison.png\")\n",
    "\n",
    "    def get_comparison_results(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Get the current comparison results.\"\"\"\n",
    "        return self.model_comparison_results.copy()\n",
    "\n",
    "    def clear_comparison_results(self):\n",
    "        \"\"\"Clear all comparison results.\"\"\"\n",
    "        self.model_comparison_results.clear()\n",
    "\n",
    "\n",
    "# Convenience functions for backward compatibility\n",
    "def evaluate_ocr_model(model, processor, dataset, output_dir=\"ocr_evaluation_results\", **kwargs):\n",
    "    \"\"\"\n",
    "    Convenience function that maintains backward compatibility with the original function.\n",
    "    \"\"\"\n",
    "    evaluator = OCRModelEvaluator()\n",
    "    return evaluator.evaluate_model(model, processor, dataset, output_dir, **kwargs)\n",
    "\n",
    "\n",
    "def create_evaluator():\n",
    "    \"\"\"Create a new OCR evaluator instance.\"\"\"\n",
    "    return OCRModelEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f7eeec-ffde-4992-86a7-fd78266219ef",
   "metadata": {
    "id": "45f7eeec-ffde-4992-86a7-fd78266219ef"
   },
   "source": [
    "# Load and finetune gema3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4777f7fe-8fda-449a-b60b-91dfaa159fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Gemma3 patching. Transformers: 4.52.4. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    #model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba132b7-c9ba-4f43-bf24-7b782dd00eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_evaluator = OCRModelEvaluator()\n",
    "model_comparison_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44cf255a-50b6-445e-85b8-1fe3d8798d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating OCR performance: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [08:01<00:00,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Summary:\n",
      "Average WER: 0.8584\n",
      "Average CER: 0.6946\n",
      "\n",
      "Detailed results saved to base_model_results/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# benchmark lora model performance\n",
    "model_name = \"Base model\" \n",
    "avg_wer, avg_cer = ocr_evaluator.evaluate_model(model=model, processor=processor, dataset=eval_dataset, top_p=0.95, top_k=64, output_dir=\"base_model_results\", max_new_tokens=64, temperature=1.0)\n",
    "ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50dda6a4-ede1-4811-82c0-e0a055d12df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have set `compile_config`, but we are unable to meet the criteria for compilation. Compilation will be skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the transcription of the text in the image:\n",
      "\n",
      "â€œBeaucoup d'entre vous savent Ã  quel point Jimmy Ã©tait pour nous, surtout sa maman.â€<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "sample = dataset[1]\n",
    "image =  sample[\"image\"].convert('RGB')\n",
    "messages =  [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": sample[\"question\"],\n",
    "                        },{\n",
    "                            \"type\": \"image\",\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(processor.tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a2deb57-cd2e-47bf-988e-919d3db4d0b2",
   "metadata": {
    "id": "3a2deb57-cd2e-47bf-988e-919d3db4d0b2",
    "outputId": "3cc5911a-cfe5-43e2-e6ca-dfa4a464dfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `base_model.model.model.vision_tower.vision_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c7c4695-e93a-4e6d-a943-690543bcbb72",
   "metadata": {
    "id": "2c7c4695-e93a-4e6d-a943-690543bcbb72"
   },
   "outputs": [],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "model.config.use_cache = False\n",
    "\n",
    "\n",
    "args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs = {\"use_reentrant\": False}, # use reentrant checkpointing\n",
    "        max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "        warmup_ratio=0.03,\n",
    "        max_steps=60,\n",
    "        #num_train_epochs = 2, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps = 5,\n",
    "        save_strategy=\"epoch\",\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"gemma3-french-ocr-checkpoints\",\n",
    "        report_to = \"none\",     # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 4,\n",
    "        max_seq_length = 2048,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37acf3c3-2804-4f95-9b78-fdec749112ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=processor.tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model,processor),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70ccb372-8d17-4076-8b2f-692fca151396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,000 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 38,497,792/4,000,000,000 (0.96% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:40, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>22.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.552700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.626100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.519600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.530200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.512800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.512800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5aa90d-2f90-4e9f-b99c-5bcba0ff68c3",
   "metadata": {
    "id": "fb5aa90d-2f90-4e9f-b99c-5bcba0ff68c3"
   },
   "source": [
    "# save qlora adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa983e6-d0f7-4b4c-a924-5612b47acb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu aurais dÃ» voir ces hommes, mÃ¨re.<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "sample=dataset[9]\n",
    "image =  sample[\"image\"].convert('RGB')\n",
    "messages =  [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": sample[\"question\"],\n",
    "                        },{\n",
    "                            \"type\": \"image\",\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(processor.tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13aaa234-8452-4a60-92bd-624b58ee91ec",
   "metadata": {
    "id": "13aaa234-8452-4a60-92bd-624b58ee91ec",
    "outputId": "22ebd55a-c605-43fa-d564-46556b506cf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unsloth-gemma3-ocr-adapter/processor_config.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"unsloth-gemma3-ocr-adapter\", processor)\n",
    "processor.save_pretrained(\"unsloth-gemma3-ocr-adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b835c5-ea65-4bbe-bc73-8a712759115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating OCR performance: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [08:26<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Summary:\n",
      "Average WER: 0.0451\n",
      "Average CER: 0.0084\n",
      "\n",
      "Detailed results saved to peft_model_results/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# benchmark lora model performance\n",
    "model_name = \"Peft model\" \n",
    "avg_wer, avg_cer = ocr_evaluator.evaluate_model(model=model, processor=processor, dataset=eval_dataset, top_p=0.95, top_k=64, output_dir=\"peft_model_results\", max_new_tokens=64, temperature=1.0)\n",
    "ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc35b36a-e1f9-4c27-812f-54c36083238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu aurais dÃ» voir ces hommes, mÃ¨re.<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!   \n",
    "\n",
    "sample=dataset[9]\n",
    "image =  sample[\"image\"].convert('RGB')\n",
    "messages =  [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": sample[\"question\"],\n",
    "                        },{\n",
    "                            \"type\": \"image\",\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(processor.tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966d45e-6c06-44fd-a98d-c07831bee864",
   "metadata": {
    "id": "c966d45e-6c06-44fd-a98d-c07831bee864"
   },
   "source": [
    "# Merge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c88e0a60-9dc9-43e5-a539-7e3430096bfa",
   "metadata": {
    "id": "c88e0a60-9dc9-43e5-a539-7e3430096bfa",
    "outputId": "7de6ef5a-95d1-4212-dddc-9c24c436f54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /mnt/disks/unslothai/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Successfully copied all 2 files from cache to gemma3-merged-finetune-merge-16bit.\n",
      "Downloading safetensors index for unsloth/gemma-3-4b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:23<00:00, 11.69s/it]\n"
     ]
    }
   ],
   "source": [
    "# merge default 16 bits\n",
    "model.save_pretrained_merged(save_directory=\"gemma3-merged-finetune-merge-16bit\", tokenizer=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfab9db-f358-4372-aece-00997ce2275f",
   "metadata": {
    "id": "ddfab9db-f358-4372-aece-00997ce2275f"
   },
   "source": [
    "# Load Merged model and benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "322a3a1f-5c5d-4a58-aad0-b348701bbfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f7fc197-1d0d-430e-b41d-33d1a9a930e5",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "933fb595cfa5475b87350d81cae515be"
     ]
    },
    "id": "8f7fc197-1d0d-430e-b41d-33d1a9a930e5",
    "outputId": "d34bbe75-1651-4238-f2e0-e207202b68e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Gemma3 patching. Transformers: 4.52.4. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c24481316e4e2f9528e0d5aafe172f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model in 4 bits\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\"./gemma3-merged-finetune-merge-16bit\",load_in_4bit=True, load_in_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d3ead64-096f-4362-bc87-8927306a6669",
   "metadata": {
    "id": "1d3ead64-096f-4362-bc87-8927306a6669"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating OCR performance: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [05:39<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Summary:\n",
      "Average WER: 0.0540\n",
      "Average CER: 0.0103\n",
      "\n",
      "Detailed results saved to merged_model_load4bits_results/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# benchmark 4bits lora model performance\n",
    "model_name = \"Merged model 4bits\" \n",
    "avg_wer, avg_cer = ocr_evaluator.evaluate_model(model=model, processor=tokenizer, dataset=eval_dataset, top_p=0.95, top_k=64, output_dir=\"merged_model_load4bits_results\", max_new_tokens=64, temperature=1.0)\n",
    "ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9a50c67-4fa7-4799-80c2-9b2b0d6b0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cec20cef-da75-4d54-8ad5-f2a9344ba6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Gemma3 patching. Transformers: 4.52.4. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3badcd592c74076b5c1acf98d91783c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model in 16 bits\n",
    "model, processor = FastVisionModel.from_pretrained(\"./gemma3-merged-finetune-merge-16bit\",load_in_4bit=False, load_in_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad8f9720-ed87-4300-8f43-106842843055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating OCR performance: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [03:14<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Summary:\n",
      "Average WER: 0.0496\n",
      "Average CER: 0.0090\n",
      "\n",
      "Detailed results saved to merged_model_load16bits_results/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# benchmark 4bits lora model performance\n",
    "model_name = \"Merged model 16bits\" \n",
    "avg_wer, avg_cer = ocr_evaluator.evaluate_model(model=model, processor=processor, dataset=eval_dataset, top_p=0.95, top_k=64, output_dir=\"merged_model_load16bits_results\", max_new_tokens=64, temperature=1.0)\n",
    "ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47177caa-8150-471a-a60e-c844d1a0275f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
